{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3O1GK7KG3GC"
      },
      "source": [
        "### Treść mini projektu 5\n",
        "\n",
        "Zadanie polega na stworzeniu modelu rekurencyjnego który przewidywał będzie kompozytora danego utworu muzyki klasycznej w oparciu o jego zapis w formie sekwencji akordów. Akordy znormalizowane zostały do klucza C-dur lub a-mol w zależności od skali utworu (durowa/molowa).\n",
        "Dane przygotowane są w postaci Pickli - https://docs.python.org/3/library/pickle.html w których znajduje się lista krotek z sekwencjami i odpowiadającymi im klasami - autorami odpowiednio: {0: 'bach', 1: 'beethoven', 2: 'debussy', 3: 'scarlatti', 4: 'victoria'} (train.pkl). W pliku test_no_target znajdują się testowe sekwencje, dla których predykcje mają Państwo przewidzieć.\n",
        "\n",
        "\n",
        "Uwaga, utwory mogą być oczywiście różnych długości. W celu stworzenia batcha danych różnej długości, muszą je Państwo odpowiednio przygotować stosując tzw. padding. Przykładowo można się posiłkować tym tutorialem: https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html. W Państwa przypadku będzie to trochę łatwiejsze bo dotyczy problemu klasyfikacji sekwencji, a nie tłumaczenia sequence-to-sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ePXzvFllG0LO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKm2Y4qwHPBu",
        "outputId": "de845147-d869-422e-e76c-297ef88291e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") \n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s4qU1BysHUEr"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wV1351u0HuNI"
      },
      "outputs": [],
      "source": [
        "with open('sample_data/train.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MwfX_FW8SLt6"
      },
      "outputs": [],
      "source": [
        "dataset[:] = [(torch.from_numpy(x[0]), x[1]) for x in dataset[:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xdiJUoKLVSP",
        "outputId": "03dcaea9-6551-4d7e-a0b3-78c458630573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2939\n",
            "[2204, 735]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(620)\n",
        "A = int(len(dataset)*0.75)\n",
        "B = len(dataset) - int(len(dataset)*0.75)\n",
        "lengths = [A, B]\n",
        "print(len(dataset))\n",
        "print(lengths)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, lengths)\n",
        "type(train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XXyAWKkfCY4w"
      },
      "outputs": [],
      "source": [
        "def pad_collate(batch):\n",
        "  (xx, yy) = zip(*batch)\n",
        "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
        "  \n",
        "  yy = torch.Tensor(yy)\n",
        "  return xx_pad, yy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zaLTvdUS70FP"
      },
      "outputs": [],
      "source": [
        "y_train_indices = train_set.indices\n",
        "\n",
        "y_train = [dataset[i][1] for i in y_train_indices]\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, sampler=sampler, num_workers = 0, collate_fn=pad_collate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IgrnU9FO_Ow4"
      },
      "outputs": [],
      "source": [
        "y_test_indices = test_set.indices\n",
        "\n",
        "y_test = [dataset[i][1] for i in y_test_indices]\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_test == t)[0]) for t in np.unique(y_test)])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in y_test])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=32,shuffle = False, sampler=sampler, num_workers = 0, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LEpdDqKM3Rx"
      },
      "outputs": [],
      "source": [
        "# train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers = 0, collate_fn=pad_collate)\n",
        "# test_loader = data.DataLoader(test_set, batch_size=32, shuffle=False, num_workers = 0, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EWOCl9-aNmLa"
      },
      "outputs": [],
      "source": [
        "class LSTMRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, out_size)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden, state\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        x = torch.transpose(x,0,1)\n",
        "        all_outputs, hidden = self.lstm(x, hidden)\n",
        "        out = all_outputs[-1]\n",
        "        x = self.fc(out)\n",
        "        return x, hidden\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeVezQiCiUhP",
        "outputId": "bb87f4f5-fb59-4347-ae8d-a6b8ff98b653"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMRegressor(\n",
              "  (lstm): LSTM(1, 50, num_layers=2)\n",
              "  (fc): Linear(in_features=50, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LSTMRegressor(1,50,2,5).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DTkp4X_afVlh"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, targets in test_loader:\n",
        "          x = x.type(torch.float).to(device).unsqueeze(2)\n",
        "          targets = targets.to(device)\n",
        "          hidden, state = model.init_hidden(x.size(0))\n",
        "          hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "          output, last_hidden = model(x, (hidden,state))\n",
        "          pred = output.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "          total += x.shape[0]\n",
        "    model.train()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIGeXUxjDkKK",
        "outputId": "db86a0d6-bd7f-4e15-97ed-5642e34e6cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss_mean: 1.59, loss: 1.56, test_acc: 0.226\n",
            "Epoch: 10, loss_mean: 1.58, loss: 1.58, test_acc: 0.23\n",
            "Epoch: 20, loss_mean: 1.56, loss: 1.64, test_acc: 0.244\n",
            "Epoch: 30, loss_mean: 1.55, loss: 1.55, test_acc: 0.229\n",
            "Epoch: 40, loss_mean: 1.58, loss: 1.55, test_acc: 0.267\n",
            "Epoch: 50, loss_mean: 1.53, loss: 1.42, test_acc: 0.282\n",
            "Epoch: 60, loss_mean: 1.53, loss: 1.49, test_acc: 0.273\n",
            "Epoch: 70, loss_mean: 1.55, loss: 1.6, test_acc: 0.267\n",
            "Epoch: 80, loss_mean: 1.54, loss: 1.73, test_acc: 0.282\n",
            "Epoch: 90, loss_mean: 1.52, loss: 1.53, test_acc: 0.288\n",
            "Epoch: 100, loss_mean: 1.52, loss: 2.1, test_acc: 0.282\n",
            "Epoch: 110, loss_mean: 1.54, loss: 1.63, test_acc: 0.287\n",
            "Epoch: 120, loss_mean: 1.54, loss: 1.47, test_acc: 0.273\n",
            "Epoch: 130, loss_mean: 1.54, loss: 1.57, test_acc: 0.265\n",
            "Epoch: 140, loss_mean: 1.53, loss: 1.5, test_acc: 0.252\n",
            "Epoch: 150, loss_mean: 1.54, loss: 1.55, test_acc: 0.261\n",
            "Epoch: 160, loss_mean: 1.53, loss: 1.49, test_acc: 0.25\n",
            "Epoch: 170, loss_mean: 1.55, loss: 1.56, test_acc: 0.261\n",
            "Epoch: 180, loss_mean: 1.55, loss: 1.54, test_acc: 0.282\n",
            "Epoch: 190, loss_mean: 1.53, loss: 1.43, test_acc: 0.301\n",
            "Epoch: 200, loss_mean: 1.53, loss: 1.47, test_acc: 0.269\n",
            "Epoch: 210, loss_mean: 1.52, loss: 1.41, test_acc: 0.278\n",
            "Epoch: 220, loss_mean: 1.54, loss: 1.49, test_acc: 0.269\n",
            "Epoch: 230, loss_mean: 1.54, loss: 2.01, test_acc: 0.309\n",
            "Epoch: 240, loss_mean: 1.52, loss: 1.54, test_acc: 0.272\n",
            "Epoch: 250, loss_mean: 1.51, loss: 1.58, test_acc: 0.294\n",
            "Epoch: 260, loss_mean: 1.53, loss: 1.43, test_acc: 0.293\n",
            "Epoch: 270, loss_mean: 1.5, loss: 1.38, test_acc: 0.331\n",
            "Epoch: 280, loss_mean: 1.34, loss: 1.34, test_acc: 0.389\n",
            "Epoch: 290, loss_mean: 1.31, loss: 1.25, test_acc: 0.358\n",
            "Epoch: 300, loss_mean: 1.17, loss: 1.06, test_acc: 0.463\n",
            "Epoch: 310, loss_mean: 1.09, loss: 1.17, test_acc: 0.483\n",
            "Epoch: 320, loss_mean: 0.957, loss: 1.15, test_acc: 0.494\n",
            "Epoch: 330, loss_mean: 0.908, loss: 0.949, test_acc: 0.574\n",
            "Epoch: 340, loss_mean: 0.864, loss: 1.03, test_acc: 0.543\n",
            "Epoch: 350, loss_mean: 0.813, loss: 0.707, test_acc: 0.559\n",
            "Epoch: 360, loss_mean: 0.821, loss: 0.757, test_acc: 0.581\n",
            "Epoch: 370, loss_mean: 0.758, loss: 0.729, test_acc: 0.588\n",
            "Epoch: 380, loss_mean: 0.722, loss: 0.763, test_acc: 0.566\n",
            "Epoch: 390, loss_mean: 0.642, loss: 0.969, test_acc: 0.57\n",
            "Epoch: 400, loss_mean: 0.633, loss: 0.409, test_acc: 0.569\n",
            "Epoch: 410, loss_mean: 0.622, loss: 0.935, test_acc: 0.578\n",
            "Epoch: 420, loss_mean: 0.639, loss: 0.786, test_acc: 0.659\n",
            "Epoch: 430, loss_mean: 0.557, loss: 0.444, test_acc: 0.633\n",
            "Epoch: 440, loss_mean: 0.524, loss: 0.288, test_acc: 0.631\n",
            "Epoch: 450, loss_mean: 0.509, loss: 0.47, test_acc: 0.595\n",
            "Epoch: 460, loss_mean: 1.45, loss: 1.44, test_acc: 0.386\n",
            "Epoch: 470, loss_mean: 0.688, loss: 0.679, test_acc: 0.571\n",
            "Epoch: 480, loss_mean: 0.587, loss: 0.469, test_acc: 0.612\n",
            "Epoch: 490, loss_mean: 0.564, loss: 0.478, test_acc: 0.648\n",
            "Epoch: 500, loss_mean: 0.497, loss: 0.658, test_acc: 0.694\n",
            "Epoch: 510, loss_mean: 0.362, loss: 0.51, test_acc: 0.667\n",
            "Epoch: 520, loss_mean: 0.282, loss: 0.166, test_acc: 0.644\n",
            "Epoch: 530, loss_mean: 0.284, loss: 0.359, test_acc: 0.695\n",
            "Epoch: 540, loss_mean: 0.255, loss: 0.567, test_acc: 0.661\n",
            "Epoch: 550, loss_mean: 0.266, loss: 0.217, test_acc: 0.684\n",
            "Epoch: 560, loss_mean: 0.195, loss: 0.315, test_acc: 0.695\n",
            "Epoch: 570, loss_mean: 0.269, loss: 0.676, test_acc: 0.641\n",
            "Epoch: 580, loss_mean: 0.148, loss: 0.273, test_acc: 0.713\n",
            "Epoch: 590, loss_mean: 0.141, loss: 0.103, test_acc: 0.672\n",
            "Epoch: 600, loss_mean: 0.15, loss: 0.398, test_acc: 0.683\n",
            "Epoch: 610, loss_mean: 0.112, loss: 0.0788, test_acc: 0.622\n",
            "Epoch: 620, loss_mean: 0.145, loss: 0.117, test_acc: 0.686\n",
            "Epoch: 630, loss_mean: 0.135, loss: 0.171, test_acc: 0.653\n",
            "Epoch: 640, loss_mean: 1.52, loss: 1.23, test_acc: 0.531\n",
            "Epoch: 650, loss_mean: 0.357, loss: 0.147, test_acc: 0.639\n",
            "Epoch: 660, loss_mean: 0.217, loss: 0.204, test_acc: 0.652\n",
            "Epoch: 670, loss_mean: 0.129, loss: 0.267, test_acc: 0.702\n",
            "Epoch: 680, loss_mean: 0.107, loss: 0.344, test_acc: 0.667\n",
            "Epoch: 690, loss_mean: 0.137, loss: 0.321, test_acc: 0.68\n",
            "Epoch: 700, loss_mean: 0.125, loss: 0.0335, test_acc: 0.673\n",
            "Epoch: 710, loss_mean: 0.101, loss: 0.0473, test_acc: 0.644\n",
            "Epoch: 720, loss_mean: 0.0966, loss: 0.0405, test_acc: 0.671\n",
            "Epoch: 730, loss_mean: 0.0678, loss: 0.025, test_acc: 0.653\n",
            "Epoch: 740, loss_mean: 0.0819, loss: 0.0995, test_acc: 0.672\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(750):\n",
        "    epoch_losses = []\n",
        "    for x, targets in train_loader:\n",
        "        x = x.type(torch.float).to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device) \n",
        "        preds, last_hidden = model(x, (hidden,state))\n",
        "        loss = loss_fun(preds, targets.type(torch.int64))\n",
        "        loss.backward()\n",
        "        epoch_losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    if epoch % 10 == 0:\n",
        "        loss_mean = np.array(epoch_losses).mean()\n",
        "        test_acc = get_accuracy(model, test_set)\n",
        "        print(f\"Epoch: {epoch}, loss_mean: {loss_mean:.3}, loss: {loss.item():.3}, test_acc: {test_acc:.3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AnnzRMMgNZ-4"
      },
      "outputs": [],
      "source": [
        "classes = ('0', '1', '2', '3','4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKw0Ws3JDSsA",
        "outputId": "045650e4-9b16-46da-9331-3b7a918dfca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for class 0     is: 81.6 %\n",
            "Accuracy for class 1     is: 68.4 %\n",
            "Accuracy for class 2     is: 44.4 %\n",
            "Accuracy for class 3     is: 73.8 %\n",
            "Accuracy for class 4     is: 73.1 %\n"
          ]
        }
      ],
      "source": [
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, targets in test_loader:\n",
        "        x = x.type(torch.float).to(device).unsqueeze(2)\n",
        "        targets = targets\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device) \n",
        "\n",
        "        outputs, last_hidden = model(x, (hidden,state))\n",
        "        outputs = outputs.cpu()\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        for target, prediction in zip(targets, predictions):\n",
        "            target = target.item()\n",
        "            prediction = prediction.item()\n",
        "            if target == prediction:\n",
        "                correct_pred[str(int(target))] += 1\n",
        "            total_pred[str(int(target))] += 1\n",
        "\n",
        "  \n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, \n",
        "                                                   accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tpa8tA9OugOk"
      },
      "outputs": [],
      "source": [
        "with open('sample_data/test_no_target.pkl', 'rb') as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "test_dataset = [(torch.from_numpy(x)) for x in test_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgWATJV0uoXJ",
        "outputId": "5eea59fe-93ed-409c-eaf5-5b1ddf88780d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f00669ed390>\n",
            "1102\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "def get_pred(model, data):\n",
        "    model.eval()\n",
        "    f = open(\"results.csv\", \"w\")\n",
        "    pred = []\n",
        "    test_all_data = torch.utils.data.DataLoader(data, batch_size=1)\n",
        "    print(test_all_data)\n",
        "    for i, data in enumerate(test_all_data, 0):\n",
        "        x = data\n",
        "        x = x.type(torch.float).to(device).unsqueeze(2)\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "        output, last_hidden = model(x, (hidden,state))\n",
        "        \n",
        "\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        f.write(\"{}\\n\".format( pred.item()))\n",
        "    f.close()\n",
        "    print(i)\n",
        "get_pred(model, test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Borowiki5wagi (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
