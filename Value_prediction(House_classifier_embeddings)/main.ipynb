{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLtBmLellVBm"
      },
      "source": [
        "SSN \n",
        "Olga Krupa\n",
        "Ewa Roszczyk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eg4gQiFQs8Ll"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data as data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1O-d4WxUtJ10"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fhgGJako2m6_"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available(): \n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZIcBb88BtdYG"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"train_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "caCJ8qZECMJv"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(columns=[\"YrSold\",\"MonthSold\",\"N_FacilitiesNearBy(Total)\",\"N_SchoolNearBy(Total)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jdp26Bwqu_6n"
      },
      "outputs": [],
      "source": [
        "categorical_columns = [\"HallwayType\", \"HeatingType\", \"AptManageType\", \"TimeToBusStop\", \"TimeToSubway\", \"SubwayStation\"]\n",
        "categorical_values = pd.get_dummies(train_data[categorical_columns])\n",
        "train_data['SalePrice'] = train_data['SalePrice'].apply(lambda b: 1 if b > 300000 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "h6KQzes8wU7r"
      },
      "outputs": [],
      "source": [
        "train_data.drop(columns=categorical_columns,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7nhP4IEW3KY7"
      },
      "outputs": [],
      "source": [
        "np.random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZzJjGM6wdkE",
        "outputId": "7afadc07-0604-4ae7-e56f-9584b29ab34d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ True,  True,  True, ..., False,  True,  True])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_indices = np.random.rand(len(train_data))>0.3\n",
        "train_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MbZg579fwpbw"
      },
      "outputs": [],
      "source": [
        "numerical_data = torch.from_numpy(train_data.values[train_indices,1:]).float()\n",
        "categorical_data = torch.from_numpy(categorical_values.values[train_indices]).float()\n",
        "targets = torch.from_numpy(train_data.values[train_indices,0]).float()\n",
        "\n",
        "test_numerical_data = torch.from_numpy(train_data.values[~train_indices,1:]).float()\n",
        "test_categorical_data = torch.from_numpy(categorical_values.values[~train_indices]).float()\n",
        "test_targets = torch.from_numpy(train_data.values[~train_indices,0]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fzoUQyqdx3Ls"
      },
      "outputs": [],
      "source": [
        "train_dataset = data.TensorDataset(numerical_data,categorical_data,targets)\n",
        "test_dataset = data.TensorDataset(test_numerical_data,test_categorical_data,test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9mQeF0V0yvBL"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval() #*********#\n",
        "    for x, cat_x, labels in data_loader:\n",
        "        x, cat_x, labels = x.to(device), cat_x.to(device), labels.to(device)\n",
        "        output = model(x, cat_x)\n",
        "        pred = output>0\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += x.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nYoZJMtX9HgI"
      },
      "outputs": [],
      "source": [
        "class House_classifier_embeddings(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(House_classifier_embeddings, self).__init__()\n",
        "        self.emb_layer = nn.Linear(categorical_data.shape[1], categorical_data.shape[1])\n",
        "        self.act_emb = nn.Tanh()\n",
        "        self.layer1 = nn.Linear(train_data.shape[1] -1 + categorical_data.shape[1], 40)\n",
        "        self.bn1 = nn.BatchNorm1d(40)\n",
        "        self.act_1 =  nn.LeakyReLU()\n",
        "        self.d1 = nn.Dropout(0.5)\n",
        "        self.layer2 = nn.Linear(40, 20)\n",
        "        self.bn2 = nn.BatchNorm1d(20)\n",
        "        self.act_2 =  nn.LeakyReLU()\n",
        "        self.d2 = nn.Dropout(0.5)\n",
        "        self.layer3 = nn.Linear(20, 1)\n",
        "    def forward(self, x, cat_x):\n",
        "        cat_x_embedded = self.emb_layer(cat_x)\n",
        "        cat_x_embedded = self.act_emb(cat_x_embedded)\n",
        "        x = torch.cat([x,cat_x_embedded],dim=1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        activation1 = self.act_1(x)\n",
        "        activation1 = self.d1(activation1)\n",
        "        x = self.layer2(activation1)\n",
        "        x = self.bn2(x)\n",
        "        activation2 = self.act_2(x)\n",
        "        activation2 = self.d2(activation2)\n",
        "        output = self.layer3(activation2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhsaM8kYzy3p",
        "outputId": "c6767649-e541-4b3c-c6d0-01da3ed17cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 0.672 test_acc: 0.751\n",
            "Epoch 1 loss 0.556 test_acc: 0.858\n",
            "Epoch 2 loss 0.477 test_acc: 0.862\n",
            "Epoch 3 loss 0.425 test_acc: 0.868\n",
            "Epoch 4 loss 0.399 test_acc: 0.866\n",
            "Epoch 5 loss 0.375 test_acc: 0.866\n",
            "Epoch 6 loss 0.365 test_acc: 0.855\n",
            "Epoch 7 loss 0.35 test_acc: 0.865\n",
            "Epoch 8 loss 0.334 test_acc: 0.869\n",
            "Epoch 9 loss 0.328 test_acc: 0.865\n",
            "Epoch 10 loss 0.323 test_acc: 0.884\n",
            "Epoch 11 loss 0.316 test_acc: 0.876\n",
            "Epoch 12 loss 0.318 test_acc: 0.865\n",
            "Epoch 13 loss 0.318 test_acc: 0.854\n",
            "Epoch 14 loss 0.312 test_acc: 0.911\n",
            "Epoch 15 loss 0.3 test_acc: 0.903\n",
            "Epoch 16 loss 0.303 test_acc: 0.915\n",
            "Epoch 17 loss 0.294 test_acc: 0.904\n",
            "Epoch 18 loss 0.3 test_acc: 0.915\n",
            "Epoch 19 loss 0.283 test_acc: 0.915\n",
            "Epoch 20 loss 0.295 test_acc: 0.921\n",
            "Epoch 21 loss 0.304 test_acc: 0.918\n",
            "Epoch 22 loss 0.29 test_acc: 0.916\n",
            "Epoch 23 loss 0.288 test_acc: 0.922\n",
            "Epoch 24 loss 0.283 test_acc: 0.914\n",
            "Epoch 25 loss 0.279 test_acc: 0.915\n",
            "Epoch 26 loss 0.272 test_acc: 0.918\n",
            "Epoch 27 loss 0.283 test_acc: 0.922\n",
            "Epoch 28 loss 0.289 test_acc: 0.914\n",
            "Epoch 29 loss 0.277 test_acc: 0.918\n",
            "Epoch 30 loss 0.273 test_acc: 0.913\n",
            "Epoch 31 loss 0.271 test_acc: 0.919\n",
            "Epoch 32 loss 0.282 test_acc: 0.914\n",
            "Epoch 33 loss 0.278 test_acc: 0.921\n",
            "Epoch 34 loss 0.261 test_acc: 0.919\n",
            "Epoch 35 loss 0.268 test_acc: 0.922\n",
            "Epoch 36 loss 0.275 test_acc: 0.923\n",
            "Epoch 37 loss 0.276 test_acc: 0.915\n",
            "Epoch 38 loss 0.261 test_acc: 0.922\n",
            "Epoch 39 loss 0.26 test_acc: 0.922\n",
            "Epoch 40 loss 0.264 test_acc: 0.922\n",
            "Epoch 41 loss 0.268 test_acc: 0.921\n",
            "Epoch 42 loss 0.255 test_acc: 0.926\n",
            "Epoch 43 loss 0.267 test_acc: 0.923\n",
            "Epoch 44 loss 0.253 test_acc: 0.926\n",
            "Epoch 45 loss 0.263 test_acc: 0.922\n",
            "Epoch 46 loss 0.256 test_acc: 0.922\n",
            "Epoch 47 loss 0.267 test_acc: 0.929\n",
            "Epoch 48 loss 0.254 test_acc: 0.923\n",
            "Epoch 49 loss 0.262 test_acc: 0.922\n",
            "Epoch 50 loss 0.258 test_acc: 0.93\n",
            "Epoch 51 loss 0.252 test_acc: 0.93\n",
            "Epoch 52 loss 0.259 test_acc: 0.926\n",
            "Epoch 53 loss 0.266 test_acc: 0.927\n",
            "Epoch 54 loss 0.261 test_acc: 0.937\n",
            "Epoch 55 loss 0.267 test_acc: 0.93\n",
            "Epoch 56 loss 0.266 test_acc: 0.931\n",
            "Epoch 57 loss 0.253 test_acc: 0.937\n",
            "Epoch 58 loss 0.259 test_acc: 0.927\n",
            "Epoch 59 loss 0.254 test_acc: 0.933\n",
            "Epoch 60 loss 0.25 test_acc: 0.934\n",
            "Epoch 61 loss 0.248 test_acc: 0.938\n",
            "Epoch 62 loss 0.255 test_acc: 0.934\n",
            "Epoch 63 loss 0.262 test_acc: 0.934\n",
            "Epoch 64 loss 0.253 test_acc: 0.934\n",
            "Epoch 65 loss 0.252 test_acc: 0.932\n",
            "Epoch 66 loss 0.252 test_acc: 0.938\n",
            "Epoch 67 loss 0.254 test_acc: 0.936\n",
            "Epoch 68 loss 0.238 test_acc: 0.938\n",
            "Epoch 69 loss 0.243 test_acc: 0.937\n",
            "Epoch 70 loss 0.256 test_acc: 0.935\n",
            "Epoch 71 loss 0.256 test_acc: 0.937\n",
            "Epoch 72 loss 0.238 test_acc: 0.936\n",
            "Epoch 73 loss 0.25 test_acc: 0.926\n",
            "Epoch 74 loss 0.24 test_acc: 0.937\n",
            "Epoch 75 loss 0.264 test_acc: 0.935\n",
            "Epoch 76 loss 0.253 test_acc: 0.928\n",
            "Epoch 77 loss 0.252 test_acc: 0.938\n",
            "Epoch 78 loss 0.251 test_acc: 0.938\n",
            "Epoch 79 loss 0.248 test_acc: 0.933\n",
            "Epoch 80 loss 0.244 test_acc: 0.938\n",
            "Epoch 81 loss 0.247 test_acc: 0.939\n",
            "Epoch 82 loss 0.244 test_acc: 0.938\n",
            "Epoch 83 loss 0.255 test_acc: 0.936\n",
            "Epoch 84 loss 0.246 test_acc: 0.938\n",
            "Epoch 85 loss 0.239 test_acc: 0.938\n",
            "Epoch 86 loss 0.241 test_acc: 0.936\n",
            "Epoch 87 loss 0.242 test_acc: 0.933\n",
            "Epoch 88 loss 0.249 test_acc: 0.938\n",
            "Epoch 89 loss 0.25 test_acc: 0.938\n",
            "Epoch 90 loss 0.237 test_acc: 0.937\n",
            "Epoch 91 loss 0.236 test_acc: 0.939\n",
            "Epoch 92 loss 0.241 test_acc: 0.937\n",
            "Epoch 93 loss 0.252 test_acc: 0.938\n",
            "Epoch 94 loss 0.239 test_acc: 0.939\n",
            "Epoch 95 loss 0.238 test_acc: 0.938\n",
            "Epoch 96 loss 0.238 test_acc: 0.938\n",
            "Epoch 97 loss 0.256 test_acc: 0.939\n",
            "Epoch 98 loss 0.247 test_acc: 0.938\n",
            "Epoch 99 loss 0.242 test_acc: 0.939\n",
            "Epoch 100 loss 0.249 test_acc: 0.939\n",
            "Epoch 101 loss 0.236 test_acc: 0.938\n",
            "Epoch 102 loss 0.246 test_acc: 0.939\n",
            "Epoch 103 loss 0.237 test_acc: 0.938\n",
            "Epoch 104 loss 0.233 test_acc: 0.936\n",
            "Epoch 105 loss 0.234 test_acc: 0.938\n",
            "Epoch 106 loss 0.235 test_acc: 0.938\n",
            "Epoch 107 loss 0.245 test_acc: 0.932\n",
            "Epoch 108 loss 0.243 test_acc: 0.937\n",
            "Epoch 109 loss 0.238 test_acc: 0.938\n",
            "Epoch 110 loss 0.245 test_acc: 0.939\n",
            "Epoch 111 loss 0.246 test_acc: 0.939\n",
            "Epoch 112 loss 0.234 test_acc: 0.939\n",
            "Epoch 113 loss 0.237 test_acc: 0.939\n",
            "Epoch 114 loss 0.239 test_acc: 0.94\n",
            "Epoch 115 loss 0.248 test_acc: 0.938\n",
            "Epoch 116 loss 0.236 test_acc: 0.939\n",
            "Epoch 117 loss 0.244 test_acc: 0.938\n",
            "Epoch 118 loss 0.239 test_acc: 0.938\n",
            "Epoch 119 loss 0.251 test_acc: 0.938\n",
            "Epoch 120 loss 0.24 test_acc: 0.937\n",
            "Epoch 121 loss 0.242 test_acc: 0.938\n",
            "Epoch 122 loss 0.239 test_acc: 0.939\n",
            "Epoch 123 loss 0.238 test_acc: 0.939\n",
            "Epoch 124 loss 0.232 test_acc: 0.939\n",
            "Epoch 125 loss 0.244 test_acc: 0.939\n",
            "Epoch 126 loss 0.251 test_acc: 0.939\n",
            "Epoch 127 loss 0.23 test_acc: 0.938\n",
            "Epoch 128 loss 0.238 test_acc: 0.938\n",
            "Epoch 129 loss 0.246 test_acc: 0.938\n",
            "Epoch 130 loss 0.237 test_acc: 0.938\n",
            "Epoch 131 loss 0.235 test_acc: 0.938\n",
            "Epoch 132 loss 0.237 test_acc: 0.934\n",
            "Epoch 133 loss 0.232 test_acc: 0.938\n",
            "Epoch 134 loss 0.247 test_acc: 0.938\n",
            "Epoch 135 loss 0.237 test_acc: 0.938\n",
            "Epoch 136 loss 0.247 test_acc: 0.938\n",
            "Epoch 137 loss 0.232 test_acc: 0.939\n",
            "Epoch 138 loss 0.235 test_acc: 0.938\n",
            "Epoch 139 loss 0.228 test_acc: 0.938\n",
            "Epoch 140 loss 0.228 test_acc: 0.939\n",
            "Epoch 141 loss 0.244 test_acc: 0.938\n",
            "Epoch 142 loss 0.233 test_acc: 0.939\n",
            "Epoch 143 loss 0.238 test_acc: 0.938\n",
            "Epoch 144 loss 0.227 test_acc: 0.939\n",
            "Epoch 145 loss 0.238 test_acc: 0.938\n",
            "Epoch 146 loss 0.231 test_acc: 0.938\n",
            "Epoch 147 loss 0.239 test_acc: 0.938\n",
            "Epoch 148 loss 0.23 test_acc: 0.939\n",
            "Epoch 149 loss 0.234 test_acc: 0.938\n",
            "Epoch 150 loss 0.234 test_acc: 0.938\n",
            "Epoch 151 loss 0.238 test_acc: 0.939\n",
            "Epoch 152 loss 0.241 test_acc: 0.938\n",
            "Epoch 153 loss 0.239 test_acc: 0.938\n",
            "Epoch 154 loss 0.231 test_acc: 0.938\n",
            "Epoch 155 loss 0.247 test_acc: 0.938\n",
            "Epoch 156 loss 0.235 test_acc: 0.939\n",
            "Epoch 157 loss 0.23 test_acc: 0.938\n",
            "Epoch 158 loss 0.245 test_acc: 0.94\n",
            "Epoch 159 loss 0.238 test_acc: 0.939\n",
            "Epoch 160 loss 0.245 test_acc: 0.939\n",
            "Epoch 161 loss 0.241 test_acc: 0.939\n",
            "Epoch 162 loss 0.224 test_acc: 0.939\n",
            "Epoch 163 loss 0.239 test_acc: 0.939\n",
            "Epoch 164 loss 0.242 test_acc: 0.938\n",
            "Epoch 165 loss 0.231 test_acc: 0.939\n",
            "Epoch 166 loss 0.245 test_acc: 0.938\n",
            "Epoch 167 loss 0.236 test_acc: 0.938\n",
            "Epoch 168 loss 0.24 test_acc: 0.938\n",
            "Epoch 169 loss 0.233 test_acc: 0.938\n",
            "Epoch 170 loss 0.235 test_acc: 0.939\n",
            "Epoch 171 loss 0.23 test_acc: 0.939\n",
            "Epoch 172 loss 0.236 test_acc: 0.938\n",
            "Epoch 173 loss 0.235 test_acc: 0.938\n",
            "Epoch 174 loss 0.228 test_acc: 0.939\n",
            "Epoch 175 loss 0.229 test_acc: 0.938\n",
            "Epoch 176 loss 0.23 test_acc: 0.938\n",
            "Epoch 177 loss 0.235 test_acc: 0.937\n",
            "Epoch 178 loss 0.239 test_acc: 0.939\n",
            "Epoch 179 loss 0.23 test_acc: 0.939\n",
            "Epoch 180 loss 0.23 test_acc: 0.938\n",
            "Epoch 181 loss 0.231 test_acc: 0.939\n",
            "Epoch 182 loss 0.221 test_acc: 0.939\n",
            "Epoch 183 loss 0.238 test_acc: 0.938\n",
            "Epoch 184 loss 0.241 test_acc: 0.938\n",
            "Epoch 185 loss 0.225 test_acc: 0.938\n",
            "Epoch 186 loss 0.22 test_acc: 0.939\n",
            "Epoch 187 loss 0.248 test_acc: 0.939\n",
            "Epoch 188 loss 0.231 test_acc: 0.938\n",
            "Epoch 189 loss 0.23 test_acc: 0.938\n",
            "Epoch 190 loss 0.23 test_acc: 0.939\n",
            "Epoch 191 loss 0.231 test_acc: 0.939\n",
            "Epoch 192 loss 0.225 test_acc: 0.938\n",
            "Epoch 193 loss 0.227 test_acc: 0.938\n",
            "Epoch 194 loss 0.244 test_acc: 0.939\n",
            "Epoch 195 loss 0.248 test_acc: 0.939\n",
            "Epoch 196 loss 0.234 test_acc: 0.94\n",
            "Epoch 197 loss 0.227 test_acc: 0.938\n",
            "Epoch 198 loss 0.226 test_acc: 0.938\n",
            "Epoch 199 loss 0.235 test_acc: 0.938\n",
            "Epoch 200 loss 0.225 test_acc: 0.938\n",
            "Epoch 201 loss 0.223 test_acc: 0.938\n",
            "Epoch 202 loss 0.228 test_acc: 0.939\n",
            "Epoch 203 loss 0.237 test_acc: 0.938\n",
            "Epoch 204 loss 0.233 test_acc: 0.938\n",
            "Epoch 205 loss 0.23 test_acc: 0.939\n",
            "Epoch 206 loss 0.232 test_acc: 0.939\n",
            "Epoch 207 loss 0.244 test_acc: 0.939\n",
            "Epoch 208 loss 0.222 test_acc: 0.939\n",
            "Epoch 209 loss 0.232 test_acc: 0.938\n",
            "Epoch 210 loss 0.238 test_acc: 0.939\n",
            "Epoch 211 loss 0.222 test_acc: 0.94\n",
            "Epoch 212 loss 0.24 test_acc: 0.939\n",
            "Epoch 213 loss 0.231 test_acc: 0.939\n",
            "Epoch 214 loss 0.237 test_acc: 0.939\n",
            "Epoch 215 loss 0.235 test_acc: 0.939\n",
            "Epoch 216 loss 0.228 test_acc: 0.938\n",
            "Epoch 217 loss 0.23 test_acc: 0.939\n",
            "Epoch 218 loss 0.238 test_acc: 0.938\n",
            "Epoch 219 loss 0.242 test_acc: 0.938\n",
            "Epoch 220 loss 0.226 test_acc: 0.938\n",
            "Epoch 221 loss 0.234 test_acc: 0.939\n",
            "Epoch 222 loss 0.233 test_acc: 0.938\n",
            "Epoch 223 loss 0.232 test_acc: 0.939\n",
            "Epoch 224 loss 0.227 test_acc: 0.939\n",
            "Epoch 225 loss 0.238 test_acc: 0.938\n",
            "Epoch 226 loss 0.235 test_acc: 0.939\n",
            "Epoch 227 loss 0.234 test_acc: 0.939\n",
            "Epoch 228 loss 0.23 test_acc: 0.939\n",
            "Epoch 229 loss 0.236 test_acc: 0.938\n",
            "Epoch 230 loss 0.221 test_acc: 0.939\n",
            "Epoch 231 loss 0.235 test_acc: 0.938\n",
            "Epoch 232 loss 0.237 test_acc: 0.938\n",
            "Epoch 233 loss 0.237 test_acc: 0.939\n",
            "Epoch 234 loss 0.234 test_acc: 0.939\n",
            "Epoch 235 loss 0.233 test_acc: 0.938\n",
            "Epoch 236 loss 0.227 test_acc: 0.938\n",
            "Epoch 237 loss 0.237 test_acc: 0.939\n",
            "Epoch 238 loss 0.23 test_acc: 0.938\n",
            "Epoch 239 loss 0.238 test_acc: 0.939\n",
            "Epoch 240 loss 0.239 test_acc: 0.939\n",
            "Epoch 241 loss 0.227 test_acc: 0.938\n",
            "Epoch 242 loss 0.233 test_acc: 0.939\n",
            "Epoch 243 loss 0.232 test_acc: 0.938\n",
            "Epoch 244 loss 0.235 test_acc: 0.939\n",
            "Epoch 245 loss 0.239 test_acc: 0.939\n",
            "Epoch 246 loss 0.233 test_acc: 0.938\n",
            "Epoch 247 loss 0.231 test_acc: 0.939\n",
            "Epoch 248 loss 0.228 test_acc: 0.938\n",
            "Epoch 249 loss 0.243 test_acc: 0.94\n",
            "Final Training Accuracy: 0.9188811188811189\n",
            "Final Validation Accuracy: 0.939873417721519\n"
          ]
        }
      ],
      "source": [
        "model = House_classifier_embeddings().to(device)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "iters = []\n",
        "losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "for n in range(250):\n",
        "    epoch_losses = []\n",
        "    for x, cat_x, labels in iter(train_loader):\n",
        "        x, cat_x, labels = x.to(device), cat_x.to(device), labels.to(device)\n",
        "        model.train() \n",
        "        out = model(x, cat_x).squeeze()           \n",
        "\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()  \n",
        "        epoch_losses.append(loss.item())\n",
        "        optimizer.step()              \n",
        "        optimizer.zero_grad()         \n",
        "\n",
        "    loss_mean = np.array(epoch_losses).mean()\n",
        "    iters.append(n)\n",
        "    losses.append(loss_mean)\n",
        "    test_acc = get_accuracy(model, test_loader)\n",
        "    print(f\"Epoch {n} loss {loss_mean:.3} test_acc: {test_acc:.3}\")\n",
        "    train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy \n",
        "    val_acc.append(test_acc)  # compute validation accuracy\n",
        "        \n",
        "\n",
        "print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UZCc7fAjYfYI"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(\"test_data.csv\")\n",
        "test_data = test_data.drop(columns=[\"YrSold\",\"MonthSold\",\"N_FacilitiesNearBy(Total)\",\"N_SchoolNearBy(Total)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "x7-kgXt_ZhFK"
      },
      "outputs": [],
      "source": [
        "categorical_columns_test = [\"HallwayType\", \"HeatingType\", \"AptManageType\", \"TimeToBusStop\", \"TimeToSubway\", \"SubwayStation\"]\n",
        "\n",
        "categorical_values_test = pd.get_dummies(test_data[categorical_columns_test])\n",
        "\n",
        "test_data.drop(columns=categorical_columns_test,inplace=True)\n",
        "test_data\n",
        "\n",
        "numerical_data_test = torch.from_numpy(test_data.values[:,:])\n",
        "categorical_data_test = torch.from_numpy(categorical_values_test.values[:,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "r40FDjBJYhiz"
      },
      "outputs": [],
      "source": [
        "preds = model(numerical_data_test.to(device).float(), categorical_data_test.to(device).float())\n",
        "pd.DataFrame(preds.cpu().detach().numpy()).to_csv(\"results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "g0aBb4zcYidW",
        "outputId": "2a0e0046-11ed-4a9a-e3bd-943f1641d7a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10.147328</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-9.173476</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-4.406162</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.876463</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.481103</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1762</th>\n",
              "      <td>-1.528491</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1763</th>\n",
              "      <td>-9.326757</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1764</th>\n",
              "      <td>-0.758648</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>1.304965</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1766</th>\n",
              "      <td>-1.382713</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1767 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0  1\n",
              "0    -10.147328  0\n",
              "1     -9.173476  0\n",
              "2     -4.406162  0\n",
              "3     -3.876463  0\n",
              "4     -1.481103  0\n",
              "...         ... ..\n",
              "1762  -1.528491  0\n",
              "1763  -9.326757  0\n",
              "1764  -0.758648  0\n",
              "1765   1.304965  1\n",
              "1766  -1.382713  0\n",
              "\n",
              "[1767 rows x 2 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv(\"results.csv\")\n",
        "res = results['0'].to_list()\n",
        "a = []\n",
        "for i in res:\n",
        "  if i > 0:\n",
        "    a.append(1)\n",
        "  else:\n",
        "    a.append(0)\n",
        "\n",
        "results['1'] = a\n",
        "\n",
        "results.to_csv(\"borowiki.csv\", index=False)\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Borowiki_projekt_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
